import os
import json
import re
import time
import requests
import opencc
import pickle
import numpy as np
import shutil
import logging
import urllib3
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from tqdm import tqdm

# ç¦ç”¨æ‰€æœ‰SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='iptv_processor.log'
)
logger = logging.getLogger(__name__)

# --- å…¨å±€é…ç½®ä¸åˆå§‹åŒ– ---
timestart = datetime.now()
print(f"ğŸš€ AIå­¦ä¹ å‹IPTVç³»ç»Ÿå¯åŠ¨ @ {timestart.strftime('%Y-%m-%d %H:%M:%S')}")
logger.info(f"ç³»ç»Ÿå¯åŠ¨ @ {timestart}")

# --- ä¿®å¤ OpenCC åŠ è½½é—®é¢˜ ---
try:
    CC_CONVERTER = opencc.OpenCC('t2s')
    print("âœ… OpenCCè½¬æ¢å™¨å·²æˆåŠŸåŠ è½½å†…ç½®é…ç½® 't2s'")
    logger.info("OpenCCè½¬æ¢å™¨å·²æˆåŠŸåŠ è½½å†…ç½®é…ç½® 't2s'")
except Exception as e:
    print(f"âŒ é”™è¯¯: åŠ è½½OpenCCè½¬æ¢å™¨å¤±è´¥ - {e}")
    logger.error(f"åŠ è½½OpenCCè½¬æ¢å™¨å¤±è´¥: {e}")
    class FallbackConverter:
        def convert(self, text):
            trad_to_simp = {
                'å»£': 'å¹¿', 'æ±': 'ä¸œ', 'è¡›': 'å«', 'è¦–': 'è§†', 'è‡º': 'å°',
                'ç£': 'æ¹¾', 'é›»': 'ç”µ', 'è¦–': 'è§†', 'é »': 'é¢‘', 'ç¶œ': 'ç»¼',
                'è—': 'è‰º', 'é«”': 'ä½“', 'è‚²': 'è‚²', 'åœ‹': 'å›½', 'éš›': 'é™…'
            }
            return ''.join(trad_to_simp.get(char, char) for char in text)
    CC_CONVERTER = FallbackConverter()
    print("âœ… ä½¿ç”¨ç®€æ˜“ç®€ç¹è½¬æ¢å™¨")
    logger.info("ä½¿ç”¨ç®€æ˜“ç®€ç¹è½¬æ¢å™¨")

# åŠ è½½åˆ†ç±»é…ç½®
try:
    with open('category_config.json', 'r', encoding='utf-8') as f:
        CATEGORY_CONFIG = json.load(f)
    print("âœ… åˆ†ç±»é…ç½®åŠ è½½æˆåŠŸ")
    logger.info("åˆ†ç±»é…ç½®åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºæ‰€æœ‰ç±»åˆ«IDçš„åˆ—è¡¨
    ALL_CATEGORIES = []
    for category_type in CATEGORY_CONFIG.values():
        for cat_id in category_type:
            ALL_CATEGORIES.append(cat_id)
    ALL_CATEGORIES.extend(['cw', 'zb', 'mv', 'radio', 'lx', 'other'])
except Exception as e:
    print(f"âŒ åŠ è½½åˆ†ç±»é…ç½®å¤±è´¥: {e}")
    logger.error(f"åŠ è½½åˆ†ç±»é…ç½®å¤±è´¥: {e}")
    exit(1)

# é…ç½® requests ä¼šè¯
def create_requests_session():
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    retries = Retry(total=5, backoff_factor=0.5, 
                   status_forcelist=[500, 502, 503, 504, 429],
                   allowed_methods=frozenset(['GET', 'POST']))
    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    session.verify = False  # ç¦ç”¨è¯ä¹¦éªŒè¯
    return session

HTTP_SESSION = create_requests_session()

# é¢‘é“åç§°è§„èŒƒåŒ–é…ç½®
REMOVAL_LIST = [
    "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[ipv6]", "[ipv4]", "_ç”µä¿¡", "ç”µä¿¡", "ï¼ˆHDï¼‰", "[è¶…æ¸…]", 
    "é«˜æ¸…", "è¶…æ¸…", "-HD", "(HK)", "AKtv", "@", "IPV6", "ğŸï¸", "ğŸ¦", " ", "[BD]", 
    "[VGA]", "[HD]", "[SD]", "(1080p)", "(720p)", "(480p)", "HD", "FHD", "4K", 
    "8K", "ç›´æ’­", "LIVE", "å®æ—¶", "RTMP", "HLS", "HTTP", "://", "è½¬ç ", "æµç•…", "æµ‹è¯•"
]
REPLACEMENTS = {
    "CCTV-": "CCTV", "CCTV0": "CCTV", "PLUS": "+", "NewTV-": "NewTV", 
    "iHOT-": "iHOT", "NEW": "New", "New_": "New", "ä¸­å¤®": "CCTV", "å¹¿ä¸œä½“è‚²å«è§†": "å¹¿ä¸œä½“è‚²",
    "æ¹–å—ç”µè§†å°": "æ¹–å—å«è§†", "æµ™æ±Ÿç”µè§†å°": "æµ™æ±Ÿå«è§†", "æ±æ–¹è¡›è¦–": "ä¸œæ–¹å«è§†", "å‡¤å‡°ä¸­æ–‡": "å‡¤å‡°å«è§†",
    "å‡¤å‡°èµ„è®¯å°": "å‡¤å‡°èµ„è®¯", "FOXä½“è‚²": "FS", "ESPNä½“è‚²": "ESPN", "DISCOVERYæ¢ç´¢": "æ¢ç´¢é¢‘é“",
    "å›½å®¶åœ°ç†é¢‘é“": "å›½å®¶åœ°ç†", "å†å²é¢‘é“": "å†å²", "HBOç”µå½±": "HBO", "CNNå›½é™…": "CNN"
}

# é¢‘é“åç§°è§„èŒƒåŒ–æ­£åˆ™
CHANNEL_PATTERNS = [
    (r'^(CCTV[\d+]+)(?:[\u4e00-\u9fa5]*)$', r'\1'),  # CCTVæ•°å­—é¢‘é“
    (r'(æ¹–å—|æµ™æ±Ÿ|æ±Ÿè‹|ä¸œæ–¹|åŒ—äº¬|å¹¿ä¸œ|æ·±åœ³|å¤©æ´¥|å±±ä¸œ)(?:å«è§†|ç”µè§†å°)', r'\1å«è§†'),
    (r'(å‡¤å‡°)(?:ä¸­æ–‡|èµ„è®¯|å«è§†)', r'\1å«è§†'),
    (r'(æ˜Ÿç©º|åå¨±|TVB|ç¿¡ç¿ |æ˜ç )(?:å«è§†|å°)', r'\1å«è§†'),
    (r'(HBO|CNN|BBC|NHK|DISCOVERY)(?:\s*[\u4e00-\u9fa5]+)?$', r'\1'),
    (r'(å«è§†|ä½“è‚²|æ–°é—»|ç”µå½±|å¨±ä¹|å¡é€š|å›½é™…|ç»¼åˆ)(?:é¢‘é“|å°)', r'\1é¢‘é“'),
]

# --- AIå­¦ä¹ æ¨¡å— ---
class AIClassifier:
    def __init__(self):
        self.model = None
        self.vectorizer = None
        self.training_data = []
        self.training_labels = []
        self.model_file = "ai_model.pkl"
        self.training_data_file = "training_data.pkl"
        self.backup_model_file = "backup_model.pkl"
        self.backup_data_file = "backup_data.pkl"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.model_file), exist_ok=True)
        os.makedirs(os.path.dirname(self.training_data_file), exist_ok=True)
        
        # å°è¯•åŠ è½½ç°æœ‰æ¨¡å‹
        if os.path.exists(self.model_file) and os.path.exists(self.training_data_file):
            try:
                with open(self.model_file, 'rb') as f:
                    self.model = pickle.load(f)
                with open(self.training_data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.training_data = data['texts']
                    self.training_labels = data['labels']
                print(f"âœ… åŠ è½½AIæ¨¡å‹ï¼Œå·²æœ‰ {len(self.training_data)} æ¡è®­ç»ƒæ•°æ®")
                logger.info(f"åŠ è½½AIæ¨¡å‹ï¼Œå·²æœ‰ {len(self.training_data)} æ¡è®­ç»ƒæ•°æ®")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹åŠ è½½é”™è¯¯: {e}")
                logger.error(f"æ¨¡å‹åŠ è½½é”™è¯¯: {e}")
                # å°è¯•ä»å¤‡ä»½æ¢å¤
                if os.path.exists(self.backup_model_file) and os.path.exists(self.backup_data_file):
                    try:
                        shutil.copy(self.backup_model_file, self.model_file)
                        shutil.copy(self.backup_data_file, self.training_data_file)
                        print("â™»ï¸ ä»å¤‡ä»½æ¢å¤æ¨¡å‹")
                        logger.info("ä»å¤‡ä»½æ¢å¤æ¨¡å‹")
                        self.__init__()  # é‡æ–°åˆå§‹åŒ–
                    except Exception as backup_e:
                        print(f"âš ï¸ å¤‡ä»½æ¢å¤å¤±è´¥: {backup_e}")
                        logger.error(f"å¤‡ä»½æ¢å¤å¤±è´¥: {backup_e}")
                        self._init_new_model()
                else:
                    print("âš ï¸ æ— å¯ç”¨å¤‡ä»½ï¼Œåˆ›å»ºæ–°æ¨¡å‹")
                    logger.info("æ— å¯ç”¨å¤‡ä»½ï¼Œåˆ›å»ºæ–°æ¨¡å‹")
                    self._init_new_model()
        else:
            print("â„¹ï¸ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œåˆ›å»ºæ–°æ¨¡å‹")
            logger.info("æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œåˆ›å»ºæ–°æ¨¡å‹")
            self._init_new_model()
    
    def _init_new_model(self):
        """åˆå§‹åŒ–æ–°çš„AIæ¨¡å‹"""
        # åˆ›å»ºç®€å•çš„æ–‡æœ¬åˆ†ç±»ç®¡é“
        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        self.model = make_pipeline(self.vectorizer, MultinomialNB())
        
        # æ·»åŠ åˆå§‹è®­ç»ƒæ•°æ®
        self._add_initial_training_data()
        print("âœ… åˆ›å»ºæ–°AIæ¨¡å‹")
        logger.info("åˆ›å»ºæ–°AIæ¨¡å‹")
    
    def _add_initial_training_data(self):
        """æ·»åŠ åˆå§‹è®­ç»ƒæ•°æ®"""
        # ä»åˆ†ç±»é…ç½®ä¸­æ·»åŠ ç¤ºä¾‹æ•°æ®
        for category_type in CATEGORY_CONFIG.values():
            for cat_id, info in category_type.items():
                # æ·»åŠ å…³é”®è¯
                for keyword in info.get("keywords", []):
                    self._add_sample(keyword, cat_id)
                
                # æ·»åŠ é¢‘é“åç§°
                for name in info.get("dictionary", []):
                    self._add_sample(name, cat_id)
        
        # æ·»åŠ ç‰¹æ®Šç±»åˆ«
        special_categories = {
            'cw': ["æ˜¥æ™š", "æ˜¥èŠ‚è”æ¬¢æ™šä¼š", "å¤®è§†æ˜¥æ™š"],
            'zb': ["ç›´æ’­ä¸­å›½", "ä¸­å›½ç›´æ’­"],
            'mv': ["éŸ³ä¹", "MTV", "æ¼”å”±ä¼š", "éŸ³ä¹ç°åœº"],
            'radio': ["å¹¿æ’­", "FM", "AM", "ç”µå°"],
            'lx': ["å›çœ‹", "é‡æ’­", "å›æ”¾", "å½•åƒ", "å½•æ’­"],
            'other': ["å…¶ä»–", "æœªçŸ¥", "æµ‹è¯•"]
        }
        
        for cat_id, examples in special_categories.items():
            for example in examples:
                self._add_sample(example, cat_id)
        
        # è®­ç»ƒåˆå§‹æ¨¡å‹
        self._train_model()
    
    def _add_sample(self, text, label):
        """æ·»åŠ å•ä¸ªè®­ç»ƒæ ·æœ¬"""
        self.training_data.append(text)
        self.training_labels.append(label)
    
    def _train_model(self):
        """è®­ç»ƒAIæ¨¡å‹"""
        if len(self.training_data) > 0:
            # è½¬æ¢æ•°æ®
            X = self.training_data
            y = self.training_labels
            
            # è®­ç»ƒæ¨¡å‹
            self.model.fit(X, y)
            
            # ä¿å­˜æ¨¡å‹
            self.save_model()
    
    def predict(self, text):
        """ä½¿ç”¨AIæ¨¡å‹é¢„æµ‹åˆ†ç±»"""
        if len(self.training_data) == 0:
            return "other"
        
        # é¢„æµ‹æ¦‚ç‡
        try:
            proba = self.model.predict_proba([text])[0]
            max_proba_idx = np.argmax(proba)
            max_proba = proba[max_proba_idx]
            
            # è·å–ç±»åˆ«
            predicted_class = self.model.classes_[max_proba_idx]
            
            # å¦‚æœç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼Œè¿”å›"other"
            if max_proba < 0.6:  # å¯è°ƒæ•´çš„ç½®ä¿¡åº¦é˜ˆå€¼
                return "other"
            
            return predicted_class
        except Exception as e:
            print(f"âš ï¸ AIé¢„æµ‹å‡ºé”™: {e}")
            logger.error(f"AIé¢„æµ‹å‡ºé”™: {e}")
            return "other"
    
    def add_feedback(self, channel_name, correct_category):
        """æ·»åŠ ç”¨æˆ·åé¦ˆæ•°æ®ç”¨äºå­¦ä¹ """
        # æ·»åŠ åˆ°è®­ç»ƒæ•°æ®
        self.training_data.append(channel_name)
        self.training_labels.append(correct_category)
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹
        self._train_model()
        
        print(f"ğŸ“ å·²å­¦ä¹ æ–°æ ·æœ¬: {channel_name[:20]}... â†’ {correct_category}")
        logger.info(f"å·²å­¦ä¹ æ–°æ ·æœ¬: {channel_name[:20]}... â†’ {correct_category}")
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        try:
            # å…ˆå¤‡ä»½æ—§æ¨¡å‹
            if os.path.exists(self.model_file):
                shutil.copy(self.model_file, self.backup_model_file)
            if os.path.exists(self.training_data_file):
                shutil.copy(self.training_data_file, self.backup_data_file)
            
            # ä¿å­˜æ¨¡å‹
            with open(self.model_file, 'wb') as f:
                pickle.dump(self.model, f)
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            with open(self.training_data_file, 'wb') as f:
                data = {
                    'texts': self.training_data,
                    'labels': self.training_labels
                }
                pickle.dump(data, f)
            
            print(f"ğŸ’¾ AIæ¨¡å‹å·²ä¿å­˜ï¼Œå½“å‰è®­ç»ƒæ•°æ®: {len(self.training_data)} æ¡")
            logger.info(f"AIæ¨¡å‹å·²ä¿å­˜ï¼Œè®­ç»ƒæ•°æ®: {len(self.training_data)} æ¡")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")

# åˆå§‹åŒ–AIåˆ†ç±»å™¨
ai_classifier = AIClassifier()

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            return [line.strip() for line in file if line.strip()]
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ '{file_name}' å‡ºé”™: {e}")
        logger.error(f"è¯»å–æ–‡ä»¶ '{file_name}' å‡ºé”™: {e}")
        return []

def load_corrections(filename):
    corrections = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    parts = line.strip().split(',')
                    if len(parts) > 1:
                        correct_name = parts[0].strip()
                        for name in parts[1:]:
                            corrections[name.strip()] = correct_name
        print(f"âœ… åŠ è½½çº é”™æ–‡ä»¶: {filename}")
        logger.info(f"åŠ è½½çº é”™æ–‡ä»¶: {filename}")
    except Exception as e:
        print(f"åŠ è½½çº é”™æ–‡ä»¶ '{filename}' å‡ºé”™: {e}")
        logger.error(f"åŠ è½½çº é”™æ–‡ä»¶ '{filename}' å‡ºé”™: {e}")
    return corrections

def fetch_source_content(url):
    try:
        response = HTTP_SESSION.get(url, timeout=15)
        response.raise_for_status()
        
        content = response.content
        text = None
        for encoding in ['utf-8', 'gbk', 'gb2312', 'iso-8859-1', 'big5']:
            try:
                text = content.decode(encoding)
                if text.strip().startswith("#EXTM3U"):
                    text = convert_m3u_to_txt(text)
                return text
            except UnicodeDecodeError:
                continue
        print(f"è­¦å‘Š: æ— æ³•è§£ç æº {url}")
        logger.warning(f"æ— æ³•è§£ç æº {url}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"å¤„ç†URLæºå‡ºé”™: {url} ({e})")
        logger.error(f"å¤„ç†URLæºå‡ºé”™: {url} ({e})")
        return None

def convert_m3u_to_txt(m3u_content):
    lines = m3u_content.strip().split('\n')
    txt_lines = []
    channel_name = ""
    for line in lines:
        line = line.strip()
        if line.startswith("#EXTM3U"):
            continue
        if line.startswith("#EXTINF"):
            # æå–é¢‘é“åç§°ï¼Œå»é™¤å¯èƒ½çš„å‚æ•°
            name_part = line.split(',', 1)[-1]
            channel_name = re.sub(r'[^,\w\s\u4e00-\u9fa5]+', '', name_part).strip()
        elif line.startswith("http") or line.startswith("rtmp"):
            if channel_name:
                txt_lines.append(f"{channel_name},{line}")
                channel_name = ""
        elif "#genre#" not in line and "," in line and re.match(r'^[^,]+,[^\s]+://[^\s]+$', line):
            txt_lines.append(line)
    return '\n'.join(txt_lines)

def normalize_channel_name(channel_name):
    """æ·±åº¦è§„èŒƒåŒ–é¢‘é“åç§°"""
    # ç®€ç¹è½¬æ¢
    try:
        simplified_name = CC_CONVERTER.convert(channel_name)
    except Exception:
        simplified_name = channel_name
    
    # ç§»é™¤æ— ç”¨å­—ç¬¦å’Œè¯è¯­
    cleaned_name = simplified_name
    for item in REMOVAL_LIST:
        cleaned_name = cleaned_name.replace(item, "")
    
    # åº”ç”¨æ­£åˆ™è§„èŒƒåŒ–
    for pattern, replacement in CHANNEL_PATTERNS:
        cleaned_name = re.sub(pattern, replacement, cleaned_name)
    
    # åº”ç”¨æ›¿æ¢è§„åˆ™
    for old, new in REPLACEMENTS.items():
        cleaned_name = cleaned_name.replace(old, new)
    
    # ç»Ÿä¸€æ ¼å¼
    cleaned_name = re.sub(r'\s+', '', cleaned_name)  # ç§»é™¤ç©ºæ ¼
    cleaned_name = re.sub(r'[^\w\u4e00-\u9fa5+]', '', cleaned_name)  # ç§»é™¤éä¸­æ–‡/è‹±æ–‡/æ•°å­—å­—ç¬¦
    
    return cleaned_name.strip()

def parse_and_clean_channels(source_content, corrections_name):
    channels = {}
    if not source_content:
        return channels
    
    lines = source_content.strip().split('\n')
    for line in lines:
        line = line.strip()
        if not line or "#genre#" in line or "#EXTINF:" in line:
            continue
        
        try:
            parts = line.split(',', 1)
            if len(parts) < 2:
                continue
                
            raw_name = parts[0].strip()
            channel_url = parts[1].split('$')[0].strip()

            if not channel_url:
                continue
            
            # æ·±åº¦è§„èŒƒåŒ–åç§°
            cleaned_name = normalize_channel_name(raw_name)
            
            # åº”ç”¨çº é”™è¯å…¸
            corrected_name = corrections_name.get(cleaned_name, cleaned_name)
            
            # åˆå¹¶ç›¸åŒé¢‘é“ä¸åŒæ¥æº
            if corrected_name in channels:
                # ä¿ç•™æ›´é•¿çš„URLï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
                if len(channel_url) > len(channels[corrected_name]):
                    channels[corrected_name] = channel_url
            else:
                channels[corrected_name] = channel_url
                
        except Exception as e:
            print(f"è§£æé¢‘é“è¡Œå¤±è´¥: {line} - {e}")
            logger.error(f"è§£æé¢‘é“è¡Œå¤±è´¥: {line} - {e}")
    return channels

def check_channel_availability(channel_info, timeout=2):
    """æ£€æŸ¥é¢‘é“å¯ç”¨æ€§å¹¶è¿”å›å»¶è¿Ÿ"""
    name, url = channel_info
    if "127.0.0.1" in url or "localhost" in url:
        return name, url, 0

    try:
        # å¯¹äºå¯èƒ½çš„å¤§æ–‡ä»¶æµï¼Œä½¿ç”¨HEADæ–¹æ³•å¯èƒ½ä¸åˆé€‚ï¼Œæ”¹ç”¨å¸¦èŒƒå›´çš„GET
        headers = {'Range': 'bytes=0-1', 'Connection': 'close'} if any(ext in url for ext in ['.m3u8', '.flv', '.ts']) else {}
        
        start_time = time.time()
        response = HTTP_SESSION.get(url, headers=headers, timeout=timeout, stream=True, verify=False)
        response.close()  # ç«‹å³å…³é—­è¿æ¥
        end_time = time.time()
        
        latency = int((end_time - start_time) * 1000)
        if response.status_code < 400 or response.status_code == 416:  # 416è¡¨ç¤ºèŒƒå›´è¯·æ±‚æˆåŠŸ
            return name, url, latency
        return name, url, -1
    except Exception:
        return name, url, -1

def classify_channel(channel_name):
    """æ™ºèƒ½åˆ†ç±»é¢‘é“"""
    # ç‰¹æ®Šé¢‘é“ä¼˜å…ˆå¤„ç†
    if "æ˜¥æ™š" in channel_name: return "cw"
    if "ç›´æ’­ä¸­å›½" in channel_name: return "zb"
    if any(kw in channel_name.lower() for kw in ["mtv", "music", "éŸ³æ¨‚", "æ¼”å”±ä¼š"]): return "mv"
    if any(kw in channel_name.lower() for kw in ["radio", "å¹¿æ’­", "fm", "am"]): return "radio"
    if any(kw in channel_name for kw in ["å›çœ‹", "é‡æ’­", "å›æ”¾", "å½•åƒ"]): return "lx"
    
    # å°è¯•ä½¿ç”¨è§„åˆ™åˆ†ç±»
    for category_type in CATEGORY_CONFIG.values():
        for category_id, info in category_type.items():
            # å…ˆæ£€æŸ¥å­—å…¸ä¸­çš„å®Œæ•´é¢‘é“åç§°
            for dict_name in info.get("dictionary", []):
                if dict_name in channel_name:
                    return category_id
            
            # å†æ£€æŸ¥å…³é”®è¯
            for keyword in info.get("keywords", []):
                if keyword in channel_name.lower():
                    return category_id
    
    # è§„åˆ™æ— æ³•åˆ†ç±»æ—¶ï¼Œä½¿ç”¨AIæ¨¡å‹
    return ai_classifier.predict(channel_name)

def sort_data(order, data):
    order_dict = {name: i for i, name in enumerate(order)}
    return sorted(data, key=lambda line: order_dict.get(line.split(',')[0], len(order)))

def balance_categories(categorized_lists):
    """å¹³è¡¡åˆ†ç±»ï¼Œé˜²æ­¢å•ä¸ªåˆ†ç±»é¢‘é“è¿‡å¤š"""
    MAX_PER_CATEGORY = 500  # å•ä¸ªåˆ†ç±»æœ€å¤§é¢‘é“æ•°
    
    for cat_id, items in list(categorized_lists.items()):
        if len(items) > MAX_PER_CATEGORY:
            print(f"âš ï¸ åˆ†ç±» {cat_id} é¢‘é“è¿‡å¤š({len(items)})ï¼Œè¿›è¡Œè‡ªåŠ¨åˆ†æµ")
            logger.warning(f"åˆ†ç±» {cat_id} é¢‘é“è¿‡å¤š({len(items)})ï¼Œè¿›è¡Œè‡ªåŠ¨åˆ†æµ")
            
            # å°†è¶…å‡ºéƒ¨åˆ†é‡æ–°åˆ†ç±»
            overflow = items[MAX_PER_CATEGORY:]
            categorized_lists[cat_id] = items[:MAX_PER_CATEGORY]
            
            for item in overflow:
                try:
                    channel_name, _ = item.split(',', 1)
                    new_cat = classify_channel(channel_name)
                    if new_cat not in categorized_lists:
                        categorized_lists[new_cat] = []
                    categorized_lists[new_cat].append(item)
                except:
                    # å¦‚æœåˆ†ç±»å¤±è´¥ï¼Œæ”¾å…¥å…¶ä»–
                    if 'other' not in categorized_lists:
                        categorized_lists['other'] = []
                    categorized_lists['other'].append(item)
    
    return categorized_lists

def save_files(categorized_lists):
    all_lines = []
    utc_time = datetime.now(timezone.utc)
    beijing_time = utc_time + timedelta(hours=8)
    version = f"{beijing_time.strftime('%Y%m%d %H:%M')},https://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8?contentid=2820180516001"
    all_lines.extend([f"æ›´æ–°æ—¶é—´,#genre#", version, ''])

    total_channels = 0

    def add_category(name, lines, dictionary=None):
        nonlocal total_channels
        if lines:
            all_lines.append(f"{name},#genre#")
            sorted_lines = sort_data(dictionary, lines) if dictionary else sorted(lines)
            all_lines.extend(sorted_lines)
            all_lines.append('')
            total_channels += len(lines)

    # æŒ‰åˆ†ç±»ä¼˜å…ˆçº§è¾“å‡º
    # 1. åœ°åŒºåˆ†ç±»
    for cat_id in CATEGORY_CONFIG.get("region_categories", {}):
        if cat_id in categorized_lists and categorized_lists[cat_id]:
            info = CATEGORY_CONFIG["region_categories"][cat_id]
            add_category(info["name"], categorized_lists[cat_id], info.get("dictionary"))
    
    # 2. å†…å®¹åˆ†ç±»
    for cat_id in CATEGORY_CONFIG.get("content_categories", {}):
        if cat_id in categorized_lists and categorized_lists[cat_id]:
            info = CATEGORY_CONFIG["content_categories"][cat_id]
            add_category(info["name"], categorized_lists[cat_id], info.get("dictionary"))
    
    # 3. ç‰¹æ®Šåˆ†ç±»
    for cat_id in CATEGORY_CONFIG.get("special_categories", {}):
        if cat_id in categorized_lists and categorized_lists[cat_id]:
            info = CATEGORY_CONFIG["special_categories"][cat_id]
            add_category(info["name"], categorized_lists[cat_id])

    final_content = "\n".join(all_lines)

    try:
        with open("live.txt", "w", encoding='utf-8') as f:
            f.write(final_content)
        print("âœ… é¢‘é“æ–‡ä»¶å·²ä¿å­˜: live.txt")
        logger.info("é¢‘é“æ–‡ä»¶å·²ä¿å­˜: live.txt")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å‡ºé”™: {e}")
        logger.error(f"ä¿å­˜æ–‡ä»¶å‡ºé”™: {e}")

    # ç”ŸæˆM3Uæ–‡ä»¶
    try:
        m3u_content = '#EXTM3U x-tvg-url="https://epg.112114.xyz/pp.xml.gz"\n'
        group_title = ""
        for line in final_content.strip().split('\n'):
            if not line.strip(): continue
            if "#genre#" in line:
                group_title = line.split(',')[0]
                continue
            if "," in line:
                name, url = line.split(',', 1)
                logo = f"https://epg.112114.xyz/logo/{name}.png"
                m3u_content += f'#EXTINF:-1 tvg-name="{name}" tvg-logo="{logo}" group-title="{group_title}",{name}\n{url}\n'
        
        with open("live.m3u", "w", encoding='utf-8') as f:
            f.write(m3u_content)
        print("âœ… M3Uæ–‡ä»¶å·²ä¿å­˜: live.m3u")
        logger.info("M3Uæ–‡ä»¶å·²ä¿å­˜: live.m3u")
    except Exception as e:
        print(f"âŒ ç”ŸæˆM3Uæ–‡ä»¶å‡ºé”™: {e}")
        logger.error(f"ç”ŸæˆM3Uæ–‡ä»¶å‡ºé”™: {e}")

    return total_channels

# --- åé¦ˆæ”¶é›†ä¸å­¦ä¹  ---

def collect_feedback(categorized_lists):
    """æ”¶é›†å¯èƒ½çš„åé¦ˆæ•°æ®ç”¨äºAIå­¦ä¹ """
    # åªæ”¶é›†"å…¶ä»–"ç±»åˆ«çš„é¢‘é“ä½œä¸ºæ½œåœ¨å­¦ä¹ æ ·æœ¬
    if 'other' not in categorized_lists:
        return
    
    # å°è¯•ä¸º"å…¶ä»–"ç±»åˆ«çš„é¢‘é“å¯»æ‰¾æ›´å¥½çš„åˆ†ç±»
    for item in categorized_lists['other']:
        try:
            channel_name, _ = item.split(',', 1)
            
            # ä½¿ç”¨AIæ¨¡å‹é¢„æµ‹ï¼ˆä¸é™åˆ¶ç½®ä¿¡åº¦ï¼‰
            ai_prediction = ai_classifier.model.predict([channel_name])[0]
            
            # å¦‚æœAIé¢„æµ‹çš„ç±»åˆ«ä¸æ˜¯"other"ï¼Œæ·»åŠ åˆ°å­¦ä¹ æ•°æ®
            if ai_prediction != 'other':
                ai_classifier.add_feedback(channel_name, ai_prediction)
                print(f"ğŸ¤– è‡ªåŠ¨å­¦ä¹ : {channel_name[:20]}... â†’ {ai_prediction}")
                logger.info(f"è‡ªåŠ¨å­¦ä¹ : {channel_name[:20]}... â†’ {ai_prediction}")
        except:
            pass

# --- ä¸»æ‰§è¡Œæµç¨‹ ---

def main():
    print("â¡ï¸ æ­¥éª¤ 1/5: åŠ è½½æœ¬åœ°èµ„æº...")
    logger.info("æ­¥éª¤ 1/5: åŠ è½½æœ¬åœ°èµ„æº")
    assets_dir = 'assets'
    os.makedirs(assets_dir, exist_ok=True)
    
    urls_file = os.path.join(assets_dir, 'urls.txt')
    corrections_file = os.path.join(assets_dir, 'corrections_name.txt')

    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»º
    if not os.path.exists(urls_file):
        with open(urls_file, 'w', encoding='utf-8') as f:
            f.write("# é»˜è®¤æº\nhttps://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8\n")
    
    if not os.path.exists(corrections_file):
        with open(corrections_file, 'w', encoding='utf-8') as f:
            f.write("# é¢‘é“åç§°çº é”™æ–‡ä»¶\nCCTV1,å¤®è§†1å°,ä¸­å¤®1å°\næ¹–å—å«è§†,æ¹–å—ç”µè§†å°\n")

    urls_to_process = read_txt_to_array(urls_file)
    corrections = load_corrections(corrections_file)

    if not urls_to_process:
        print(f"âŒ é”™è¯¯: '{urls_file}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        logger.error(f"'{urls_file}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡º")
        return

    print(f"\nâ¡ï¸ æ­¥éª¤ 2/5: è·å– {len(urls_to_process)} ä¸ªåœ¨çº¿æº...")
    logger.info(f"æ­¥éª¤ 2/5: è·å– {len(urls_to_process)} ä¸ªåœ¨çº¿æº")
    all_raw_channels = {}
    for url in tqdm(urls_to_process, desc="å¤„ç†æº"):
        try:
            content = fetch_source_content(url)
            if content:
                parsed_channels = parse_and_clean_channels(content, corrections)
                # åˆå¹¶ç›¸åŒé¢‘é“
                for name, url in parsed_channels.items():
                    if name in all_raw_channels:
                        # ä¿ç•™æ›´é•¿çš„URL
                        if len(url) > len(all_raw_channels[name]):
                            all_raw_channels[name] = url
                    else:
                        all_raw_channels[name] = url
        except Exception as e:
            print(f"âŒ å¤„ç†æºå¤±è´¥: {url} - {e}")
            logger.error(f"å¤„ç†æºå¤±è´¥: {url} - {e}")
    
    print(f"\nâœ… æˆåŠŸè·å–å¹¶è§£æäº† {len(all_raw_channels)} ä¸ªä¸é‡å¤çš„é¢‘é“ã€‚")
    logger.info(f"æˆåŠŸè·å–å¹¶è§£æäº† {len(all_raw_channels)} ä¸ªä¸é‡å¤çš„é¢‘é“")

    if not all_raw_channels:
        print("âŒ é”™è¯¯: æ²¡æœ‰è·å–åˆ°ä»»ä½•é¢‘é“ï¼Œç¨‹åºé€€å‡º")
        logger.error("æ²¡æœ‰è·å–åˆ°ä»»ä½•é¢‘é“ï¼Œç¨‹åºé€€å‡º")
        return

    print(f"\nâ¡ï¸ æ­¥éª¤ 3/5: æ£€æµ‹ {len(all_raw_channels)} ä¸ªé¢‘é“çš„æœ‰æ•ˆæ€§...")
    logger.info(f"æ­¥éª¤ 3/5: æ£€æµ‹ {len(all_raw_channels)} ä¸ªé¢‘é“çš„æœ‰æ•ˆæ€§")
    valid_channels = []
    total = len(all_raw_channels)
    
    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = {executor.submit(check_channel_availability, (name, url)): (name, url) 
                   for name, url in all_raw_channels.items()}
        
        for future in tqdm(as_completed(futures), total=total, desc="æ£€æµ‹é¢‘é“"):
            try:
                name, url, latency = future.result()
                if latency >= 0:
                    valid_channels.append((name, url))
            except Exception:
                pass
    
    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(valid_channels)} ä¸ªæœ‰æ•ˆé¢‘é“ã€‚")
    logger.info(f"æ£€æµ‹å®Œæˆï¼Œæœ‰æ•ˆé¢‘é“: {len(valid_channels)}")

    print("\nâ¡ï¸ æ­¥éª¤ 4/5: æ™ºèƒ½åˆ†ç±»é¢‘é“...")
    logger.info("æ­¥éª¤ 4/5: æ™ºèƒ½åˆ†ç±»é¢‘é“")
    categorized_lists = {}
    
    for name, url in tqdm(valid_channels, desc="åˆ†ç±»é¢‘é“"):
        category = classify_channel(name)
        if category not in categorized_lists:
            categorized_lists[category] = []
        categorized_lists[category].append(f"{name},{url}")
    
    # å¹³è¡¡åˆ†ç±»
    categorized_lists = balance_categories(categorized_lists)
    
    # æ”¶é›†åé¦ˆç”¨äºå­¦ä¹ 
    collect_feedback(categorized_lists)
    
    # æ‰“å°åˆ†ç±»ç»Ÿè®¡
    print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    logger.info("åˆ†ç±»ç»Ÿè®¡:")
    for cat_id, channels in categorized_lists.items():
        cat_name = "å…¶ä»–"
        # åœ¨æ‰€æœ‰åˆ†ç±»ç±»å‹ä¸­æŸ¥æ‰¾åç§°
        for cat_type in CATEGORY_CONFIG.values():
            if cat_id in cat_type:
                cat_name = cat_type[cat_id]["name"]
                break
        print(f"  - {cat_name}: {len(channels)} ä¸ªé¢‘é“")
        logger.info(f"  - {cat_name}: {len(channels)} ä¸ªé¢‘é“")
    print("âœ… åˆ†ç±»å®Œæˆã€‚")
    logger.info("åˆ†ç±»å®Œæˆ")

    print("\nâ¡ï¸ æ­¥éª¤ 5/5: ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶...")
    logger.info("æ­¥éª¤ 5/5: ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶")
    total_saved = save_files(categorized_lists)

    print("\n--- ä»»åŠ¡å®Œæˆ ---")
    timeend = datetime.now()
    elapsed = timeend - timestart
    minutes, seconds = divmod(int(elapsed.total_seconds()), 60)
    print(f"ğŸ“Š æ€»è€—æ—¶: {minutes}åˆ† {seconds}ç§’")
    print(f"ğŸ“Š æ€»è®¡æœ‰æ•ˆé¢‘é“æ•°: {total_saved}")
    logger.info(f"æ€»è€—æ—¶: {minutes}åˆ† {seconds}ç§’")
    logger.info(f"æ€»è®¡æœ‰æ•ˆé¢‘é“æ•°: {total_saved}")
    
    # ä¿å­˜AIæ¨¡å‹çŠ¶æ€
    ai_classifier.save_model()

if __name__ == "__main__":
    main()
