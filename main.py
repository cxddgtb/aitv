import os
import json
import re
import time
import requests
import opencc
import pickle
import numpy as np
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from tqdm import tqdm

# --- å…¨å±€é…ç½®ä¸åˆå§‹åŒ– ---
timestart = datetime.now()
print(f"ğŸš€ AIå­¦ä¹ å‹IPTVç³»ç»Ÿå¯åŠ¨ @ {timestart.strftime('%Y-%m-%d %H:%M:%S')}")

# --- ä¿®å¤ OpenCC åŠ è½½é—®é¢˜ ---
try:
    CC_CONVERTER = opencc.OpenCC('t2s')
    print("âœ… OpenCCè½¬æ¢å™¨å·²æˆåŠŸåŠ è½½å†…ç½®é…ç½® 't2s'")
except Exception as e:
    print(f"âŒ é”™è¯¯: åŠ è½½OpenCCè½¬æ¢å™¨å¤±è´¥ - {e}")
    class FallbackConverter:
        def convert(self, text):
            trad_to_simp = {
                'å»£': 'å¹¿', 'æ±': 'ä¸œ', 'è¡›': 'å«', 'è¦–': 'è§†', 'è‡º': 'å°',
                'ç£': 'æ¹¾', 'é›»': 'ç”µ', 'è¦–': 'è§†', 'é »': 'é¢‘', 'ç¶œ': 'ç»¼',
                'è—': 'è‰º', 'é«”': 'ä½“', 'è‚²': 'è‚²', 'åœ‹': 'å›½', 'éš›': 'é™…'
            }
            return ''.join(trad_to_simp.get(char, char) for char in text)
    CC_CONVERTER = FallbackConverter()
    print("âœ… ä½¿ç”¨ç®€æ˜“ç®€ç¹è½¬æ¢å™¨")

# åŠ è½½åˆ†ç±»é…ç½®
with open('category_config.json', 'r', encoding='utf-8') as f:
    CATEGORY_CONFIG = json.load(f)

# åˆ›å»ºæ‰€æœ‰ç±»åˆ«IDçš„åˆ—è¡¨
ALL_CATEGORIES = []
for category_type in CATEGORY_CONFIG.values():
    for cat_id in category_type:
        ALL_CATEGORIES.append(cat_id)
ALL_CATEGORIES.extend(['cw', 'zb', 'mv', 'radio', 'lx', 'other'])

# é…ç½® requests ä¼šè¯
def create_requests_session():
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    })
    retries = Retry(total=5, backoff_factor=0.5, 
                   status_forcelist=[500, 502, 503, 504, 429],
                   allowed_methods=frozenset(['GET', 'POST']))
    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

HTTP_SESSION = create_requests_session()

# é¢‘é“åç§°è§„èŒƒåŒ–é…ç½®
REMOVAL_LIST = [
    "ã€ŒIPV4ã€", "ã€ŒIPV6ã€", "[ipv6]", "[ipv4]", "_ç”µä¿¡", "ç”µä¿¡", "ï¼ˆHDï¼‰", "[è¶…æ¸…]", 
    "é«˜æ¸…", "è¶…æ¸…", "-HD", "(HK)", "AKtv", "@", "IPV6", "ğŸï¸", "ğŸ¦", " ", "[BD]", 
    "[VGA]", "[HD]", "[SD]", "(1080p)", "(720p)", "(480p)", "HD", "FHD", "4K", 
    "8K", "ç›´æ’­", "LIVE", "å®æ—¶", "RTMP", "HLS", "HTTP", "://", "è½¬ç ", "æµç•…", "æµ‹è¯•"
]
REPLACEMENTS = {
    "CCTV-": "CCTV", "CCTV0": "CCTV", "PLUS": "+", "NewTV-": "NewTV", 
    "iHOT-": "iHOT", "NEW": "New", "New_": "New", "ä¸­å¤®": "CCTV", "å¹¿ä¸œä½“è‚²å«è§†": "å¹¿ä¸œä½“è‚²",
    "æ¹–å—ç”µè§†å°": "æ¹–å—å«è§†", "æµ™æ±Ÿç”µè§†å°": "æµ™æ±Ÿå«è§†", "æ±æ–¹è¡›è¦–": "ä¸œæ–¹å«è§†", "å‡¤å‡°ä¸­æ–‡": "å‡¤å‡°å«è§†",
    "å‡¤å‡°èµ„è®¯å°": "å‡¤å‡°èµ„è®¯", "FOXä½“è‚²": "FS", "ESPNä½“è‚²": "ESPN", "DISCOVERYæ¢ç´¢": "æ¢ç´¢é¢‘é“",
    "å›½å®¶åœ°ç†é¢‘é“": "å›½å®¶åœ°ç†", "å†å²é¢‘é“": "å†å²", "HBOç”µå½±": "HBO", "CNNå›½é™…": "CNN"
}

# é¢‘é“åç§°è§„èŒƒåŒ–æ­£åˆ™
CHANNEL_PATTERNS = [
    (r'^(CCTV[\d+]+)(?:[\u4e00-\u9fa5]*)$', r'\1'),  # CCTVæ•°å­—é¢‘é“
    (r'(æ¹–å—|æµ™æ±Ÿ|æ±Ÿè‹|ä¸œæ–¹|åŒ—äº¬|å¹¿ä¸œ|æ·±åœ³|å¤©æ´¥|å±±ä¸œ)(?:å«è§†|ç”µè§†å°)', r'\1å«è§†'),
    (r'(å‡¤å‡°)(?:ä¸­æ–‡|èµ„è®¯|å«è§†)', r'\1å«è§†'),
    (r'(æ˜Ÿç©º|åå¨±|TVB|ç¿¡ç¿ |æ˜ç )(?:å«è§†|å°)', r'\1å«è§†'),
    (r'(HBO|CNN|BBC|NHK|DISCOVERY)(?:\s*[\u4e00-\u9fa5]+)?$', r'\1'),
    (r'(å«è§†|ä½“è‚²|æ–°é—»|ç”µå½±|å¨±ä¹|å¡é€š|å›½é™…|ç»¼åˆ)(?:é¢‘é“|å°)', r'\1é¢‘é“'),
]

# --- AIå­¦ä¹ æ¨¡å— ---
class AIClassifier:
    def __init__(self):
        self.model = None
        self.vectorizer = None
        self.training_data = []
        self.training_labels = []
        self.model_file = "ai_model.pkl"
        self.training_data_file = "training_data.pkl"
        
        # å°è¯•åŠ è½½ç°æœ‰æ¨¡å‹
        if os.path.exists(self.model_file) and os.path.exists(self.training_data_file):
            try:
                with open(self.model_file, 'rb') as f:
                    self.model = pickle.load(f)
                with open(self.training_data_file, 'rb') as f:
                    data = pickle.load(f)
                    self.training_data = data['texts']
                    self.training_labels = data['labels']
                print(f"âœ… åŠ è½½AIæ¨¡å‹ï¼Œå·²æœ‰ {len(self.training_data)} æ¡è®­ç»ƒæ•°æ®")
            except:
                print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ¨¡å‹")
                self._init_new_model()
        else:
            self._init_new_model()
    
    def _init_new_model(self):
        """åˆå§‹åŒ–æ–°çš„AIæ¨¡å‹"""
        # åˆ›å»ºç®€å•çš„æ–‡æœ¬åˆ†ç±»ç®¡é“
        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
        self.model = make_pipeline(self.vectorizer, MultinomialNB())
        
        # æ·»åŠ åˆå§‹è®­ç»ƒæ•°æ®
        self._add_initial_training_data()
        print("âœ… åˆ›å»ºæ–°AIæ¨¡å‹")
    
    def _add_initial_training_data(self):
        """æ·»åŠ åˆå§‹è®­ç»ƒæ•°æ®"""
        # ä»åˆ†ç±»é…ç½®ä¸­æ·»åŠ ç¤ºä¾‹æ•°æ®
        for category_type in CATEGORY_CONFIG.values():
            for cat_id, info in category_type.items():
                # æ·»åŠ å…³é”®è¯
                for keyword in info.get("keywords", []):
                    self.training_data.append(keyword)
                    self.training_labels.append(cat_id)
                
                # æ·»åŠ é¢‘é“åç§°
                for name in info.get("dictionary", []):
                    self.training_data.append(name)
                    self.training_labels.append(cat_id)
        
        # æ·»åŠ ç‰¹æ®Šç±»åˆ«
        special_categories = {
            'cw': ["æ˜¥æ™š", "æ˜¥èŠ‚è”æ¬¢æ™šä¼š", "å¤®è§†æ˜¥æ™š"],
            'zb': ["ç›´æ’­ä¸­å›½", "ä¸­å›½ç›´æ’­"],
            'mv': ["éŸ³ä¹", "MTV", "æ¼”å”±ä¼š", "éŸ³ä¹ç°åœº"],
            'radio': ["å¹¿æ’­", "FM", "AM", "ç”µå°"],
            'lx': ["å›çœ‹", "é‡æ’­", "å›æ”¾", "å½•åƒ", "å½•æ’­"]
        }
        
        for cat_id, examples in special_categories.items():
            for example in examples:
                self.training_data.append(example)
                self.training_labels.append(cat_id)
        
        # è®­ç»ƒåˆå§‹æ¨¡å‹
        self._train_model()
    
    def _train_model(self):
        """è®­ç»ƒAIæ¨¡å‹"""
        if len(self.training_data) > 0:
            # è½¬æ¢æ•°æ®
            X = self.training_data
            y = self.training_labels
            
            # è®­ç»ƒæ¨¡å‹
            self.model.fit(X, y)
            
            # ä¿å­˜æ¨¡å‹
            self.save_model()
    
    def predict(self, text):
        """ä½¿ç”¨AIæ¨¡å‹é¢„æµ‹åˆ†ç±»"""
        if len(self.training_data) == 0:
            return "other"
        
        # é¢„æµ‹æ¦‚ç‡
        proba = self.model.predict_proba([text])[0]
        max_proba_idx = np.argmax(proba)
        max_proba = proba[max_proba_idx]
        
        # è·å–ç±»åˆ«
        predicted_class = self.model.classes_[max_proba_idx]
        
        # å¦‚æœç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼Œè¿”å›"other"
        if max_proba < 0.6:  # å¯è°ƒæ•´çš„ç½®ä¿¡åº¦é˜ˆå€¼
            return "other"
        
        return predicted_class
    
    def add_feedback(self, channel_name, correct_category):
        """æ·»åŠ ç”¨æˆ·åé¦ˆæ•°æ®ç”¨äºå­¦ä¹ """
        # æ·»åŠ åˆ°è®­ç»ƒæ•°æ®
        self.training_data.append(channel_name)
        self.training_labels.append(correct_category)
        
        # é‡æ–°è®­ç»ƒæ¨¡å‹
        self._train_model()
        
        print(f"ğŸ“ å·²å­¦ä¹ æ–°æ ·æœ¬: {channel_name} â†’ {correct_category}")
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶"""
        # ä¿å­˜æ¨¡å‹
        with open(self.model_file, 'wb') as f:
            pickle.dump(self.model, f)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        with open(self.training_data_file, 'wb') as f:
            data = {
                'texts': self.training_data,
                'labels': self.training_labels
            }
            pickle.dump(data, f)

# åˆå§‹åŒ–AIåˆ†ç±»å™¨
ai_classifier = AIClassifier()

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def read_txt_to_array(file_name):
    try:
        with open(file_name, 'r', encoding='utf-8') as file:
            return [line.strip() for line in file if line.strip()]
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶ '{file_name}' å‡ºé”™: {e}")
        return []

def load_corrections(filename):
    corrections = {}
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    parts = line.strip().split(',')
                    if len(parts) > 1:
                        correct_name = parts[0].strip()
                        for name in parts[1:]:
                            corrections[name.strip()] = correct_name
    except Exception as e:
        print(f"åŠ è½½çº é”™æ–‡ä»¶ '{filename}' å‡ºé”™: {e}")
    return corrections

def fetch_source_content(url):
    try:
        response = HTTP_SESSION.get(url, timeout=15)
        response.raise_for_status()
        
        content = response.content
        text = None
        for encoding in ['utf-8', 'gbk', 'gb2312', 'iso-8859-1', 'big5']:
            try:
                text = content.decode(encoding)
                if text.strip().startswith("#EXTM3U"):
                    text = convert_m3u_to_txt(text)
                return text
            except UnicodeDecodeError:
                continue
        print(f"è­¦å‘Š: æ— æ³•è§£ç æº {url}")
        return None
    except requests.exceptions.RequestException as e:
        print(f"å¤„ç†URLæºå‡ºé”™: {url} ({e})")
        return None

def convert_m3u_to_txt(m3u_content):
    lines = m3u_content.strip().split('\n')
    txt_lines = []
    channel_name = ""
    for line in lines:
        line = line.strip()
        if line.startswith("#EXTM3U"):
            continue
        if line.startswith("#EXTINF"):
            # æå–é¢‘é“åç§°ï¼Œå»é™¤å¯èƒ½çš„å‚æ•°
            name_part = line.split(',', 1)[-1]
            channel_name = re.sub(r'[^,\w\s\u4e00-\u9fa5]+', '', name_part).strip()
        elif line.startswith("http") or line.startswith("rtmp"):
            if channel_name:
                txt_lines.append(f"{channel_name},{line}")
                channel_name = ""
        elif "#genre#" not in line and "," in line and re.match(r'^[^,]+,[^\s]+://[^\s]+$', line):
            txt_lines.append(line)
    return '\n'.join(txt_lines)

def normalize_channel_name(channel_name):
    """æ·±åº¦è§„èŒƒåŒ–é¢‘é“åç§°"""
    # ç®€ç¹è½¬æ¢
    try:
        simplified_name = CC_CONVERTER.convert(channel_name)
    except Exception:
        simplified_name = channel_name
    
    # ç§»é™¤æ— ç”¨å­—ç¬¦å’Œè¯è¯­
    cleaned_name = simplified_name
    for item in REMOVAL_LIST:
        cleaned_name = cleaned_name.replace(item, "")
    
    # åº”ç”¨æ­£åˆ™è§„èŒƒåŒ–
    for pattern, replacement in CHANNEL_PATTERNS:
        cleaned_name = re.sub(pattern, replacement, cleaned_name)
    
    # åº”ç”¨æ›¿æ¢è§„åˆ™
    for old, new in REPLACEMENTS.items():
        cleaned_name = cleaned_name.replace(old, new)
    
    # ç»Ÿä¸€æ ¼å¼
    cleaned_name = re.sub(r'\s+', '', cleaned_name)  # ç§»é™¤ç©ºæ ¼
    cleaned_name = re.sub(r'[^\w\u4e00-\u9fa5+]', '', cleaned_name)  # ç§»é™¤éä¸­æ–‡/è‹±æ–‡/æ•°å­—å­—ç¬¦
    
    return cleaned_name.strip()

def parse_and_clean_channels(source_content, corrections_name):
    channels = {}
    if not source_content:
        return channels
    
    lines = source_content.strip().split('\n')
    for line in lines:
        line = line.strip()
        if not line or "#genre#" in line or "#EXTINF:" in line:
            continue
        
        try:
            parts = line.split(',', 1)
            if len(parts) < 2:
                continue
                
            raw_name = parts[0].strip()
            channel_url = parts[1].split('$')[0].strip()

            if not channel_url:
                continue
            
            # æ·±åº¦è§„èŒƒåŒ–åç§°
            cleaned_name = normalize_channel_name(raw_name)
            
            # åº”ç”¨çº é”™è¯å…¸
            corrected_name = corrections_name.get(cleaned_name, cleaned_name)
            
            # åˆå¹¶ç›¸åŒé¢‘é“ä¸åŒæ¥æº
            if corrected_name in channels:
                # ä¿ç•™æ›´é•¿çš„URLï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
                if len(channel_url) > len(channels[corrected_name]):
                    channels[corrected_name] = channel_url
            else:
                channels[corrected_name] = channel_url
                
        except Exception as e:
            print(f"è§£æé¢‘é“è¡Œå¤±è´¥: {line} - {e}")
    return channels

def check_channel_availability(channel_info, timeout=2):
    """æ£€æŸ¥é¢‘é“å¯ç”¨æ€§å¹¶è¿”å›å»¶è¿Ÿ"""
    name, url = channel_info
    if "127.0.0.1" in url or "localhost" in url:
        return name, url, 0

    try:
        # å¯¹äºå¯èƒ½çš„å¤§æ–‡ä»¶æµï¼Œä½¿ç”¨HEADæ–¹æ³•å¯èƒ½ä¸åˆé€‚ï¼Œæ”¹ç”¨å¸¦èŒƒå›´çš„GET
        headers = {'Range': 'bytes=0-1', 'Connection': 'close'} if any(ext in url for ext in ['.m3u8', '.flv', '.ts']) else {}
        
        start_time = time.time()
        response = HTTP_SESSION.get(url, headers=headers, timeout=timeout, stream=True, verify=False)
        response.close()  # ç«‹å³å…³é—­è¿æ¥
        end_time = time.time()
        
        latency = int((end_time - start_time) * 1000)
        if response.status_code < 400 or response.status_code == 416:  # 416è¡¨ç¤ºèŒƒå›´è¯·æ±‚æˆåŠŸ
            return name, url, latency
        return name, url, -1
    except Exception:
        return name, url, -1

def classify_channel(channel_name):
    """æ™ºèƒ½åˆ†ç±»é¢‘é“"""
    # ç‰¹æ®Šé¢‘é“ä¼˜å…ˆå¤„ç†
    if "æ˜¥æ™š" in channel_name: return "cw"
    if "ç›´æ’­ä¸­å›½" in channel_name: return "zb"
    if any(kw in channel_name.lower() for kw in ["mtv", "music", "éŸ³æ¨‚", "æ¼”å”±ä¼š"]): return "mv"
    if any(kw in channel_name.lower() for kw in ["radio", "å¹¿æ’­", "fm", "am"]): return "radio"
    if any(kw in channel_name for kw in ["å›çœ‹", "é‡æ’­", "å›æ”¾", "å½•åƒ"]): return "lx"
    
    # å°è¯•ä½¿ç”¨è§„åˆ™åˆ†ç±»
    for category_type in CATEGORY_CONFIG.values():
        for category_id, info in category_type.items():
            # å…ˆæ£€æŸ¥å­—å…¸ä¸­çš„å®Œæ•´é¢‘é“åç§°
            for dict_name in info.get("dictionary", []):
                if dict_name in channel_name:
                    return category_id
            
            # å†æ£€æŸ¥å…³é”®è¯
            for keyword in info.get("keywords", []):
                if keyword in channel_name.lower():
                    return category_id
    
    # è§„åˆ™æ— æ³•åˆ†ç±»æ—¶ï¼Œä½¿ç”¨AIæ¨¡å‹
    return ai_classifier.predict(channel_name)

def sort_data(order, data):
    order_dict = {name: i for i, name in enumerate(order)}
    return sorted(data, key=lambda line: order_dict.get(line.split(',')[0], len(order)))

def save_files(categorized_lists):
    all_lines = []
    utc_time = datetime.now(timezone.utc)
    beijing_time = utc_time + timedelta(hours=8)
    version = f"{beijing_time.strftime('%Y%m%d %H:%M')},https://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8?contentid=2820180516001"
    all_lines.extend([f"æ›´æ–°æ—¶é—´,#genre#", version, ''])

    total_channels = 0

    def add_category(name, lines, dictionary=None):
        nonlocal total_channels
        if lines:
            all_lines.append(f"{name},#genre#")
            sorted_lines = sort_data(dictionary, lines) if dictionary else sorted(lines)
            all_lines.extend(sorted_lines)
            all_lines.append('')
            total_channels += len(lines)

    # æŒ‰åˆ†ç±»ä¼˜å…ˆçº§è¾“å‡º
    for category_type in CATEGORY_CONFIG.values():
        for cat_id, info in category_type.items():
            if cat_id in categorized_lists and categorized_lists[cat_id]:
                add_category(info["name"], categorized_lists[cat_id], info.get("dictionary"))
    
    # ç‰¹æ®Šç±»åˆ«
    add_category("æ˜¥æ™š", categorized_lists.get('cw', []))
    add_category("ç›´æ’­ä¸­å›½", categorized_lists.get('zb', []))
    add_category("éŸ³ä¹MV", categorized_lists.get('mv', []))
    add_category("æ”¶éŸ³æœºé¢‘é“", categorized_lists.get('radio', []))
    add_category("å½•åƒå›æ”¾", categorized_lists.get('lx', []))
    add_category("å…¶ä»–é¢‘é“", categorized_lists.get('other', []))

    final_content = "\n".join(all_lines)

    try:
        with open("live.txt", "w", encoding='utf-8') as f:
            f.write(final_content)
        print("âœ… é¢‘é“æ–‡ä»¶å·²ä¿å­˜: live.txt")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å‡ºé”™: {e}")

    # ç”ŸæˆM3Uæ–‡ä»¶
    try:
        m3u_content = '#EXTM3U x-tvg-url="https://epg.112114.xyz/pp.xml.gz"\n'
        group_title = ""
        for line in final_content.strip().split('\n'):
            if not line.strip(): continue
            if "#genre#" in line:
                group_title = line.split(',')[0]
                continue
            if "," in line:
                name, url = line.split(',', 1)
                logo = f"https://epg.112114.xyz/logo/{name}.png"
                m3u_content += f'#EXTINF:-1 tvg-name="{name}" tvg-logo="{logo}" group-title="{group_title}",{name}\n{url}\n'
        
        with open("live.m3u", "w", encoding='utf-8') as f:
            f.write(m3u_content)
        print("âœ… M3Uæ–‡ä»¶å·²ä¿å­˜: live.m3u")
    except Exception as e:
        print(f"âŒ ç”ŸæˆM3Uæ–‡ä»¶å‡ºé”™: {e}")

    return total_channels

# --- åé¦ˆæ”¶é›†ä¸å­¦ä¹  ---

def collect_feedback(categorized_lists):
    """æ”¶é›†å¯èƒ½çš„åé¦ˆæ•°æ®ç”¨äºAIå­¦ä¹ """
    # åªæ”¶é›†"å…¶ä»–"ç±»åˆ«çš„é¢‘é“ä½œä¸ºæ½œåœ¨å­¦ä¹ æ ·æœ¬
    if 'other' not in categorized_lists:
        return
    
    # å°è¯•ä¸º"å…¶ä»–"ç±»åˆ«çš„é¢‘é“å¯»æ‰¾æ›´å¥½çš„åˆ†ç±»
    for item in categorized_lists['other']:
        try:
            channel_name, _ = item.split(',', 1)
            
            # ä½¿ç”¨AIæ¨¡å‹é¢„æµ‹ï¼ˆä¸é™åˆ¶ç½®ä¿¡åº¦ï¼‰
            ai_prediction = ai_classifier.model.predict([channel_name])[0]
            
            # å¦‚æœAIé¢„æµ‹çš„ç±»åˆ«ä¸æ˜¯"other"ï¼Œæ·»åŠ åˆ°å­¦ä¹ æ•°æ®
            if ai_prediction != 'other':
                ai_classifier.add_feedback(channel_name, ai_prediction)
                print(f"ğŸ¤– è‡ªåŠ¨å­¦ä¹ : {channel_name[:20]}... â†’ {ai_prediction}")
        except:
            pass

# --- ä¸»æ‰§è¡Œæµç¨‹ ---

def main():
    print("â¡ï¸ æ­¥éª¤ 1/5: åŠ è½½æœ¬åœ°èµ„æº...")
    assets_dir = 'assets'
    os.makedirs(assets_dir, exist_ok=True)
    
    urls_file = os.path.join(assets_dir, 'urls.txt')
    corrections_file = os.path.join(assets_dir, 'corrections_name.txt')

    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»º
    if not os.path.exists(urls_file):
        with open(urls_file, 'w', encoding='utf-8') as f:
            f.write("# é»˜è®¤æº\nhttps://gcalic.v.myalicdn.com/gc/wgw05_1/index.m3u8\n")
    
    if not os.path.exists(corrections_file):
        with open(corrections_file, 'w', encoding='utf-8') as f:
            f.write("# é¢‘é“åç§°çº é”™æ–‡ä»¶\nCCTV1,å¤®è§†1å°,ä¸­å¤®1å°\næ¹–å—å«è§†,æ¹–å—ç”µè§†å°\n")

    urls_to_process = read_txt_to_array(urls_file)
    corrections = load_corrections(corrections_file)

    if not urls_to_process:
        print(f"âŒ é”™è¯¯: '{urls_file}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    print(f"\nâ¡ï¸ æ­¥éª¤ 2/5: è·å– {len(urls_to_process)} ä¸ªåœ¨çº¿æº...")
    all_raw_channels = {}
    for url in tqdm(urls_to_process, desc="å¤„ç†æº"):
        try:
            content = fetch_source_content(url)
            if content:
                parsed_channels = parse_and_clean_channels(content, corrections)
                # åˆå¹¶ç›¸åŒé¢‘é“
                for name, url in parsed_channels.items():
                    if name in all_raw_channels:
                        # ä¿ç•™æ›´é•¿çš„URL
                        if len(url) > len(all_raw_channels[name]):
                            all_raw_channels[name] = url
                    else:
                        all_raw_channels[name] = url
        except Exception as e:
            print(f"âŒ å¤„ç†æºå¤±è´¥: {url} - {e}")
    
    print(f"\nâœ… æˆåŠŸè·å–å¹¶è§£æäº† {len(all_raw_channels)} ä¸ªä¸é‡å¤çš„é¢‘é“ã€‚")

    if not all_raw_channels:
        print("âŒ é”™è¯¯: æ²¡æœ‰è·å–åˆ°ä»»ä½•é¢‘é“ï¼Œç¨‹åºé€€å‡º")
        return

    print(f"\nâ¡ï¸ æ­¥éª¤ 3/5: æ£€æµ‹ {len(all_raw_channels)} ä¸ªé¢‘é“çš„æœ‰æ•ˆæ€§...")
    valid_channels = []
    total = len(all_raw_channels)
    
    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = {executor.submit(check_channel_availability, (name, url)): (name, url) 
                   for name, url in all_raw_channels.items()}
        
        for future in tqdm(as_completed(futures), total=total, desc="æ£€æµ‹é¢‘é“"):
            try:
                name, url, latency = future.result()
                if latency >= 0:
                    valid_channels.append((name, url))
            except Exception:
                pass
    
    print(f"\nâœ… æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(valid_channels)} ä¸ªæœ‰æ•ˆé¢‘é“ã€‚")

    print("\nâ¡ï¸ æ­¥éª¤ 4/5: æ™ºèƒ½åˆ†ç±»é¢‘é“...")
    categorized_lists = {}
    
    for name, url in tqdm(valid_channels, desc="åˆ†ç±»é¢‘é“"):
        category = classify_channel(name)
        if category not in categorized_lists:
            categorized_lists[category] = []
        categorized_lists[category].append(f"{name},{url}")
    
    # æ”¶é›†åé¦ˆç”¨äºå­¦ä¹ 
    collect_feedback(categorized_lists)
    
    # æ‰“å°åˆ†ç±»ç»Ÿè®¡
    print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    for cat_id, channels in categorized_lists.items():
        cat_name = "å…¶ä»–"
        for category_type in CATEGORY_CONFIG.values():
            for cid, info in category_type.items():
                if cid == cat_id:
                    cat_name = info["name"]
                    break
        print(f"  - {cat_name}: {len(channels)} ä¸ªé¢‘é“")
    print("âœ… åˆ†ç±»å®Œæˆã€‚")

    print("\nâ¡ï¸ æ­¥éª¤ 5/5: ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶...")
    total_saved = save_files(categorized_lists)

    print("\n--- ä»»åŠ¡å®Œæˆ ---")
    timeend = datetime.now()
    elapsed = timeend - timestart
    minutes, seconds = divmod(int(elapsed.total_seconds()), 60)
    print(f"ğŸ“Š æ€»è€—æ—¶: {minutes}åˆ† {seconds}ç§’")
    print(f"ğŸ“Š æ€»è®¡æœ‰æ•ˆé¢‘é“æ•°: {total_saved}")
    
    # ä¿å­˜AIæ¨¡å‹çŠ¶æ€
    ai_classifier.save_model()
    print(f"ğŸ’¾ AIæ¨¡å‹å·²ä¿å­˜ï¼Œå½“å‰è®­ç»ƒæ•°æ®: {len(ai_classifier.training_data)} æ¡")

if __name__ == "__main__":
    main()
